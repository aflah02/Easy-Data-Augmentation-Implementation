{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EDA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPVV8XgSs8lplHHejBIKz+b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aflah02/Easy-Data-Augmentation-Implementation/blob/main/EDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "import random"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-LuYDsQtSDk",
        "outputId": "bbce2bb2-c05f-447b-e110-0e9b1b152f39"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/omw-1.4.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "r3iSEz0crpcJ"
      },
      "outputs": [],
      "source": [
        "def eda_SR(originalSentence, n):\n",
        "  \"\"\"\n",
        "  Paper Methodology -> Randomly choose n words from the sentence that are not stop words. \n",
        "                       Replace each of these words with one of its synonyms chosen at random.\n",
        "  originalSentence -> The sentence on which EDA is to be applied\n",
        "  n -> The number of words to be chosen for random synonym replacement\n",
        "  \"\"\"\n",
        "  stops = set(stopwords.words('english'))\n",
        "  splitSentence = list(originalSentence.split(\" \"))\n",
        "  splitSentenceCopy = splitSentence.copy()\n",
        "  # Since We Make Changes to The Original Sentence List The Indexes Change and Hence an initial copy proves useful to get values\n",
        "  ls_nonStopWordIndexes = []\n",
        "  for i in range(len(splitSentence)):\n",
        "    if splitSentence[i].lower() not in stops:\n",
        "      ls_nonStopWordIndexes.append(i)\n",
        "  if (n > len(ls_nonStopWordIndexes)):\n",
        "    raise Exception(\"The number of replacements exceeds the number of non stop word words\")\n",
        "  for i in range(n):\n",
        "    indexChosen = random.choice(ls_nonStopWordIndexes)\n",
        "    ls_nonStopWordIndexes.remove(indexChosen)\n",
        "    synonyms = []\n",
        "    originalWord = splitSentenceCopy[indexChosen]\n",
        "    for synset in wordnet.synsets(originalWord):\n",
        "      for lemma in synset.lemmas():\n",
        "        if lemma.name() != originalWord:\n",
        "          synonyms.append(lemma.name())\n",
        "    splitSentence[indexChosen] = random.choice(synonyms).replace('_', ' ')\n",
        "  return \" \".join(splitSentence)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(eda_SR(\"I love to play football\", 2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9S1DukZer1Am",
        "outputId": "7dad50ec-ae0c-4c3a-f193-da65fe511c88"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I dear to play football game\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def eda_RI(originalSentence, n):\n",
        "  \"\"\"\n",
        "  Paper Methodology -> Find a random synonym of a random word in the sentence that is not a stop word. \n",
        "                       Insert that synonym into a random position in the sentence. Do this n times\n",
        "  originalSentence -> The sentence on which EDA is to be applied\n",
        "  n -> The number of times the process has to be repeated\n",
        "  \"\"\"\n",
        "  stops = set(stopwords.words('english'))\n",
        "  splitSentence = list(originalSentence.split(\" \"))\n",
        "  splitSentenceCopy = splitSentence.copy() \n",
        "  # Since We Make Changes to The Original Sentence List The Indexes Change and Hence an initial copy proves useful to get values\n",
        "  ls_nonStopWordIndexes = []\n",
        "  for i in range(len(splitSentence)):\n",
        "    if splitSentence[i].lower() not in stops:\n",
        "      ls_nonStopWordIndexes.append(i)\n",
        "  if (n > len(ls_nonStopWordIndexes)):\n",
        "    raise Exception(\"The number of replacements exceeds the number of non stop word words\")\n",
        "  WordCount = len(splitSentence)\n",
        "  for i in range(n):\n",
        "    indexChosen = random.choice(ls_nonStopWordIndexes)\n",
        "    ls_nonStopWordIndexes.remove(indexChosen)\n",
        "    synonyms = []\n",
        "    originalWord = splitSentenceCopy[indexChosen]\n",
        "    for synset in wordnet.synsets(originalWord):\n",
        "      for lemma in synset.lemmas():\n",
        "        if lemma.name() != originalWord:\n",
        "          synonyms.append(lemma.name())\n",
        "    splitSentence.insert(random.randint(0,WordCount-1), random.choice(synonyms).replace('_', ' '))\n",
        "  return \" \".join(splitSentence)"
      ],
      "metadata": {
        "id": "ki7HhW0LuK32"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(eda_RI(\"I love to play football\", 2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqWbchb_uS6-",
        "outputId": "d9bb790c-462d-42f9-d81e-4e5bd5e81383"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I dear recreate love to play football\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def eda_RS(originalSentence, n):\n",
        "  \"\"\"\n",
        "  Paper Methodology -> Find a random synonym of a random word in the sentence that is not a stop word. \n",
        "                       Insert that synonym into a random position in the sentence. Do this n times\n",
        "  originalSentence -> The sentence on which EDA is to be applied\n",
        "  n -> The number of times the process has to be repeated\n",
        "  \"\"\"\n",
        "  splitSentence = list(originalSentence.split(\" \"))\n",
        "  WordCount = len(splitSentence)\n",
        "  if (WordCount == 1):\n",
        "    raise Exception(\"No Swaps Possible in One Word Sentences\")\n",
        "  for i in range(n):\n",
        "    firstIndex = random.randint(0,WordCount-1)\n",
        "    secondIndex = random.randint(0,WordCount-1)\n",
        "    while (secondIndex == firstIndex):\n",
        "      secondIndex = random.randint(0,WordCount-1)\n",
        "    splitSentence[firstIndex], splitSentence[secondIndex] = splitSentence[secondIndex], splitSentence[firstIndex]\n",
        "  return \" \".join(splitSentence)"
      ],
      "metadata": {
        "id": "vNxMDuWEyGWw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(eda_RS(\"I love to play football\", 2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSJVxXjK1XLD",
        "outputId": "0612275f-14b0-4dd2-8b70-4b84fe6391d6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I to love football play\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def eda_RD(originalSentence, p):\n",
        "  \"\"\"\n",
        "  Paper Methodology -> Randomly remove each word in the sentence with probability p.\n",
        "  originalSentence -> The sentence on which EDA is to be applied\n",
        "  p -> Probability of a Word Being Removed\n",
        "  \"\"\"\n",
        "  if (p == 1):\n",
        "      raise Exception(\"Always an Empty String Will Be Returned\") \n",
        "  if (p > 1 or p < 0):\n",
        "    raise Exception(\"Improper Probability Value\")\n",
        "  splitSentence = list(originalSentence.split(\" \"))\n",
        "  lsIndexesRemoved = []\n",
        "  WordCount = len(splitSentence)\n",
        "  if (WordCount == 1):\n",
        "    raise Exception(\"No Swaps Possible in One Word Sentences\")\n",
        "  for i in range(WordCount):\n",
        "    randomDraw = random.random()\n",
        "    if randomDraw <= p:\n",
        "      lsIndexesRemoved.append(i)\n",
        "  lsRetainingWords = []\n",
        "  for i in range(len(splitSentence)):\n",
        "    if i not in lsIndexesRemoved:\n",
        "      lsRetainingWords.append(splitSentence[i])\n",
        "  return \" \".join(lsRetainingWords)"
      ],
      "metadata": {
        "id": "J7Ftz9dw1aU7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(eda_RD(\"I love to play football\", 0.3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3BexJBe3GiU",
        "outputId": "d3fb0934-bb2c-469d-a51e-a08b05c0e0f9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I love to play football\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building Dataset"
      ],
      "metadata": {
        "id": "WJ7lKLxQX1Y7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://raw.githubusercontent.com/clairett/pytorch-sentiment-classification/master/data/SST2/train.tsv\n",
        "!wget -q https://raw.githubusercontent.com/clairett/pytorch-sentiment-classification/master/data/SST2/test.tsv\n",
        "!wget -q https://raw.githubusercontent.com/clairett/pytorch-sentiment-classification/master/data/SST2/dev.tsv"
      ],
      "metadata": {
        "id": "CxBtup4GaBfU"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir train"
      ],
      "metadata": {
        "id": "ozpfDrPTwN75"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df_train = pd.read_csv('train.tsv', delimiter = '\\t', names = ['Sentence', 'Label'])\n",
        "# df_train['Split'] = 'Train'\n",
        "df_dev = pd.read_csv('dev.tsv', delimiter = '\\t', names = ['Sentence', 'Label'])\n",
        "# df_dev['Split'] = 'Train' # Since The Original Dataset Does Not Use an Explicit Dev Set It's Considered To Be A Part of Test Set\n",
        "df_test = pd.read_csv('test.tsv', delimiter = '\\t', names = ['Sentence', 'Label'])\n",
        "# df_test['Split'] = 'Test'\n",
        "df_train = pd.concat([df_train, df_dev])\n",
        "df_train = df_train.reset_index(drop=True)\n",
        "df_train = df_train.loc[:, [\"Label\", \"Sentence\"]]\n",
        "df_train.to_csv('datasettrain.csv', index=False)\n",
        "df_test = df_test.loc[:, [\"Label\", \"Sentence\"]]\n",
        "df_test.to_csv('datasettest.csv', index=False)"
      ],
      "metadata": {
        "id": "NKufD3ezZxVL"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import pickle"
      ],
      "metadata": {
        "id": "XXWZxsCsgoO2"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "column_names = ['Label','Sentence']\n",
        "batchsize = 1\n",
        "label = column_names[0]\n",
        "train_dataset = tf.data.experimental.make_csv_dataset(\n",
        "    'datasettrain.csv',\n",
        "    batchsize,\n",
        "    column_names = column_names,\n",
        "    label_name = label,\n",
        "    num_epochs=1\n",
        ")"
      ],
      "metadata": {
        "id": "0J8dkX9H8MJV"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "column_names = ['Label','Sentence']\n",
        "batchsize = 32\n",
        "label = column_names[0]\n",
        "test_dataset = tf.data.experimental.make_csv_dataset(\n",
        "    'datasettest.csv',\n",
        "    batchsize,\n",
        "    column_names = column_names,\n",
        "    label_name = label,\n",
        "    num_epochs=1\n",
        ")"
      ],
      "metadata": {
        "id": "1QGaBd4J-a3_"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(train_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmpg0pAYJi3g",
        "outputId": "a49fcbd1-788f-4780-c517-270132d3b0de"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'tensorflow.python.data.ops.dataset_ops.PrefetchDataset'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls = []\n",
        "for example, label in train_dataset:\n",
        "  ls.append([example['Sentence'][0],label[0]])\n",
        "  # ls.append(tf.constant(example['Sentence'][0],label[0]))"
      ],
      "metadata": {
        "id": "YTBxrkYnEYlq"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSYI8NIKS203",
        "outputId": "1c347790-b989-4631-af61-38cea162da4e"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Tensor: shape=(), dtype=string, numpy=b'it has more than a few moments that are insightful enough to be fondly remembered in the endlessly challenging maze of moviegoing'>,\n",
              " <tf.Tensor: shape=(), dtype=int32, numpy=1>]"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6xwi78GNS2uB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(ls[0][0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giwrMWNEQI0x",
        "outputId": "cf9dfe9e-fd13-4480-b257-810373a8a0f5"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = tf.data.Dataset.from_tensors(ls[0][0], ls[0][1])\n",
        "dataset2 = tf.data.Dataset.from_tensors(ls[1][0], ls[1][1])\n",
        "dataset = dataset.concatenate(dataset2)\n",
        "print(dataset)\n",
        "# for i in range(1, len(ls)):\n",
        "#   dataset.concatenate(tf.Variable(ls[i][0], ls[i][1]))\n",
        "# print(type(dataset))\n",
        "dataset3 = tf.data.Dataset.sample_from_datasets(dataset2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fG_9HGjpNE3e",
        "outputId": "deb6d107-a4bc-4c2e-b795-0614b915f272"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<ConcatenateDataset element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = 10000\n",
        "BATCH_SIZE = 64"
      ],
      "metadata": {
        "id": "qqxNTN7HL09k"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rank_2_tensor = tf.constant([[1, 1]*len(ls)], dtype=tf.float16)"
      ],
      "metadata": {
        "id": "7yM_dtR9L11y"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(rank_2_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sePsa6O5MrDQ",
        "outputId": "986918fa-a8fa-421c-cefc-7908b772ea26"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensorflow.python.framework.ops.EagerTensor"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RNN Model\n",
        "\n",
        "https://www.tensorflow.org/text/tutorials/text_classification_rnn\n",
        "\n",
        "Paper Uses - \n",
        "The architecture used in this paper is as follows: input layer, bi-directional hidden layer with 64 LSTM cells, dropout layer with p=0.5, bi-directional layer of 32 LSTM cells, dropout layer with p=0.5, dense layer of 20 hidden units with ReLU activation, softmax output layer. We initialize this network with random normal weights and train against the categorical crossentropy loss function with the adam optimizer. We use early stopping with a patience of 3 epochs."
      ],
      "metadata": {
        "id": "JPXHX5UV78ui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential()\n",
        "model.add(layers.Bidirectional(layers.LSTM(64, return_sequences=True)))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Bidirectional(layers.LSTM(32, return_sequences=False)))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(20, activation='relu'))\n",
        "model.add(layers.Dense(2, kernel_initializer='normal', activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "bx_aazpwWe1f"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# callbacks = [EarlyStopping(monitor='val_loss', patience=3)]\n",
        "\n",
        "# model.fit(\t\n",
        "#       dataset,\n",
        "#       epochs=3, \n",
        "#       callbacks=callbacks,\n",
        "#       batch_size=1024, \n",
        "# )"
      ],
      "metadata": {
        "id": "Bn39t9aqrp6o"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qjBusEOGuHK3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}