{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EDA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNWIum2nOmUasMMWDpuqjEk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aflah02/Easy-Data-Augmentation-Implementation/blob/main/EDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "import random"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-LuYDsQtSDk",
        "outputId": "8ab639e6-b3fb-4722-bc58-96192ddba142"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/omw-1.4.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "r3iSEz0crpcJ"
      },
      "outputs": [],
      "source": [
        "def eda_SR(originalSentence, n):\n",
        "  \"\"\"\n",
        "  Paper Methodology -> Randomly choose n words from the sentence that are not stop words. \n",
        "                       Replace each of these words with one of its synonyms chosen at random.\n",
        "  originalSentence -> The sentence on which EDA is to be applied\n",
        "  n -> The number of words to be chosen for random synonym replacement\n",
        "  \"\"\"\n",
        "  stops = set(stopwords.words('english'))\n",
        "  splitSentence = list(originalSentence.split(\" \"))\n",
        "  splitSentenceCopy = splitSentence.copy()\n",
        "  # Since We Make Changes to The Original Sentence List The Indexes Change and Hence an initial copy proves useful to get values\n",
        "  ls_nonStopWordIndexes = []\n",
        "  for i in range(len(splitSentence)):\n",
        "    if splitSentence[i].lower() not in stops:\n",
        "      ls_nonStopWordIndexes.append(i)\n",
        "  if (n > len(ls_nonStopWordIndexes)):\n",
        "    raise Exception(\"The number of replacements exceeds the number of non stop word words\")\n",
        "  for i in range(n):\n",
        "    indexChosen = random.choice(ls_nonStopWordIndexes)\n",
        "    ls_nonStopWordIndexes.remove(indexChosen)\n",
        "    synonyms = []\n",
        "    originalWord = splitSentenceCopy[indexChosen]\n",
        "    for synset in wordnet.synsets(originalWord):\n",
        "      for lemma in synset.lemmas():\n",
        "        if lemma.name() != originalWord:\n",
        "          synonyms.append(lemma.name())\n",
        "    splitSentence[indexChosen] = random.choice(synonyms).replace('_', ' ')\n",
        "  return \" \".join(splitSentence)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(eda_SR(\"I love to play football\", 2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9S1DukZer1Am",
        "outputId": "e15e814c-15ae-4100-997c-8e854c32cb46"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I have sex to romp football\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def eda_RI(originalSentence, n):\n",
        "  \"\"\"\n",
        "  Paper Methodology -> Find a random synonym of a random word in the sentence that is not a stop word. \n",
        "                       Insert that synonym into a random position in the sentence. Do this n times\n",
        "  originalSentence -> The sentence on which EDA is to be applied\n",
        "  n -> The number of times the process has to be repeated\n",
        "  \"\"\"\n",
        "  stops = set(stopwords.words('english'))\n",
        "  splitSentence = list(originalSentence.split(\" \"))\n",
        "  splitSentenceCopy = splitSentence.copy() \n",
        "  # Since We Make Changes to The Original Sentence List The Indexes Change and Hence an initial copy proves useful to get values\n",
        "  ls_nonStopWordIndexes = []\n",
        "  for i in range(len(splitSentence)):\n",
        "    if splitSentence[i].lower() not in stops:\n",
        "      ls_nonStopWordIndexes.append(i)\n",
        "  if (n > len(ls_nonStopWordIndexes)):\n",
        "    raise Exception(\"The number of replacements exceeds the number of non stop word words\")\n",
        "  WordCount = len(splitSentence)\n",
        "  for i in range(n):\n",
        "    indexChosen = random.choice(ls_nonStopWordIndexes)\n",
        "    ls_nonStopWordIndexes.remove(indexChosen)\n",
        "    synonyms = []\n",
        "    originalWord = splitSentenceCopy[indexChosen]\n",
        "    for synset in wordnet.synsets(originalWord):\n",
        "      for lemma in synset.lemmas():\n",
        "        if lemma.name() != originalWord:\n",
        "          synonyms.append(lemma.name())\n",
        "    splitSentence.insert(random.randint(0,WordCount-1), random.choice(synonyms).replace('_', ' '))\n",
        "  return \" \".join(splitSentence)"
      ],
      "metadata": {
        "id": "ki7HhW0LuK32"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(eda_RI(\"I love to play football\", 2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqWbchb_uS6-",
        "outputId": "e846821e-0091-45a7-de38-fc10666d7f4c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I love to making love play football game football\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def eda_RS(originalSentence, n):\n",
        "  \"\"\"\n",
        "  Paper Methodology -> Find a random synonym of a random word in the sentence that is not a stop word. \n",
        "                       Insert that synonym into a random position in the sentence. Do this n times\n",
        "  originalSentence -> The sentence on which EDA is to be applied\n",
        "  n -> The number of times the process has to be repeated\n",
        "  \"\"\"\n",
        "  splitSentence = list(originalSentence.split(\" \"))\n",
        "  WordCount = len(splitSentence)\n",
        "  if (WordCount == 1):\n",
        "    raise Exception(\"No Swaps Possible in One Word Sentences\")\n",
        "  for i in range(n):\n",
        "    firstIndex = random.randint(0,WordCount-1)\n",
        "    secondIndex = random.randint(0,WordCount-1)\n",
        "    while (secondIndex == firstIndex):\n",
        "      secondIndex = random.randint(0,WordCount-1)\n",
        "    splitSentence[firstIndex], splitSentence[secondIndex] = splitSentence[secondIndex], splitSentence[firstIndex]\n",
        "  return \" \".join(splitSentence)"
      ],
      "metadata": {
        "id": "vNxMDuWEyGWw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(eda_RS(\"I love to play football\", 2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSJVxXjK1XLD",
        "outputId": "f1168789-06fd-441c-f6cb-a258b34e3427"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "play I to love football\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def eda_RD(originalSentence, p):\n",
        "  \"\"\"\n",
        "  Paper Methodology -> Randomly remove each word in the sentence with probability p.\n",
        "  originalSentence -> The sentence on which EDA is to be applied\n",
        "  p -> Probability of a Word Being Removed\n",
        "  \"\"\"\n",
        "  if (p == 1):\n",
        "      raise Exception(\"Always an Empty String Will Be Returned\") \n",
        "  if (p > 1 or p < 0):\n",
        "    raise Exception(\"Improper Probability Value\")\n",
        "  splitSentence = list(originalSentence.split(\" \"))\n",
        "  lsIndexesRemoved = []\n",
        "  WordCount = len(splitSentence)\n",
        "  if (WordCount == 1):\n",
        "    raise Exception(\"No Swaps Possible in One Word Sentences\")\n",
        "  for i in range(WordCount):\n",
        "    randomDraw = random.random()\n",
        "    if randomDraw <= p:\n",
        "      lsIndexesRemoved.append(i)\n",
        "  lsRetainingWords = []\n",
        "  for i in range(len(splitSentence)):\n",
        "    if i not in lsIndexesRemoved:\n",
        "      lsRetainingWords.append(splitSentence[i])\n",
        "  return \" \".join(lsRetainingWords)"
      ],
      "metadata": {
        "id": "J7Ftz9dw1aU7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(eda_RD(\"I love to play football\", 0.3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3BexJBe3GiU",
        "outputId": "6489e7dc-e0ff-4385-efad-049dde10cabc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "football\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building Dataset"
      ],
      "metadata": {
        "id": "WJ7lKLxQX1Y7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://raw.githubusercontent.com/clairett/pytorch-sentiment-classification/master/data/SST2/train.tsv\n",
        "!wget -q https://raw.githubusercontent.com/clairett/pytorch-sentiment-classification/master/data/SST2/test.tsv\n",
        "!wget -q https://raw.githubusercontent.com/clairett/pytorch-sentiment-classification/master/data/SST2/dev.tsv"
      ],
      "metadata": {
        "id": "CxBtup4GaBfU"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir train"
      ],
      "metadata": {
        "id": "ozpfDrPTwN75"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df_train = pd.read_csv('train.tsv', delimiter = '\\t', names = ['Sentence', 'Label'])\n",
        "df_train['Split'] = 'Train'\n",
        "df_dev = pd.read_csv('dev.tsv', delimiter = '\\t', names = ['Sentence', 'Label'])\n",
        "df_dev['Split'] = 'Test' # Since The Original Dataset Does Not Use an Explicit Dev Set It's Considered To Be A Part of Test Set\n",
        "df_test = pd.read_csv('test.tsv', delimiter = '\\t', names = ['Sentence', 'Label'])\n",
        "df_test['Split'] = 'Test'\n",
        "df = pd.concat([df_train, df_dev, df_test])\n",
        "df = df.reset_index(drop=True)\n",
        "df = df.loc[:, [\"Label\",\"Split\",\"Sentence\"]]\n",
        "print(df)\n",
        "df.to_csv('dataset.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKufD3ezZxVL",
        "outputId": "b4814585-ef53-403a-e4e4-cb81d00cb5a9"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Label  Split                                           Sentence\n",
            "0         1  Train  a stirring , funny and finally transporting re...\n",
            "1         0  Train  apparently reassembled from the cutting room f...\n",
            "2         0  Train  they presume their audience wo n't sit still f...\n",
            "3         1  Train  this is a visually stunning rumination on love...\n",
            "4         1  Train  jonathan parker 's bartleby should have been t...\n",
            "...     ...    ...                                                ...\n",
            "9608      0   Test  an often deadly boring , strange reading of a ...\n",
            "9609      0   Test  the problem with concept films is that if the ...\n",
            "9610      0   Test  safe conduct , however ambitious and well inte...\n",
            "9611      0   Test  a film made with as little wit , interest , an...\n",
            "9612      0   Test  but here 's the real damn it is n't funny , ei...\n",
            "\n",
            "[9613 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import pickle"
      ],
      "metadata": {
        "id": "XXWZxsCsgoO2"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_train(line):\n",
        "    split_line = tf.strings.split(line, \",\", maxsplit=3)\n",
        "    dataset_belonging = split_line[2]  \n",
        "    sentiment_category = split_line[1]  \n",
        "\n",
        "    return (\n",
        "        True\n",
        "        if dataset_belonging == \"Train\"\n",
        "        else False\n",
        "    )\n",
        "\n",
        "\n",
        "def filter_test(line):\n",
        "    split_line = tf.strings.split(line, \",\", maxsplit=3)\n",
        "    dataset_belonging = split_line[2] \n",
        "    sentiment_category = split_line[1] \n",
        "\n",
        "    return (\n",
        "        True if dataset_belonging == \"Test\"\n",
        "        else False\n",
        "    )"
      ],
      "metadata": {
        "id": "GHunHvKkipWw"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_train = tf.data.TextLineDataset('dataset.csv').filter(filter_train)\n",
        "ds_test = tf.data.TextLineDataset('dataset.csv').filter(filter_test)"
      ],
      "metadata": {
        "id": "G_uODGeIh6Zl"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for line in ds_test.skip(1).take(2):\n",
        "  print(line)"
      ],
      "metadata": {
        "id": "fas4BSvAiRMn"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Vocabulary"
      ],
      "metadata": {
        "id": "18l_xKj-j5-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tfds.deprecated.text.Tokenizer()"
      ],
      "metadata": {
        "id": "01EM_SP1j87z"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocabulary(ds_train, threshold=200):\n",
        "    \"\"\" Build a vocabulary \"\"\"\n",
        "    frequencies = {}\n",
        "    vocabulary = set()\n",
        "    vocabulary.update([\"sostoken\"])\n",
        "    vocabulary.update([\"eostoken\"])\n",
        "\n",
        "    for line in ds_train.skip(1):\n",
        "        split_line = tf.strings.split(line, \",\", maxsplit=3)\n",
        "        review = split_line[-1]\n",
        "        tokenized_text = tokenizer.tokenize(review.numpy().lower())\n",
        "\n",
        "        for word in tokenized_text:\n",
        "            if word not in frequencies:\n",
        "                frequencies[word] = 1\n",
        "\n",
        "            else:\n",
        "                frequencies[word] += 1\n",
        "\n",
        "            # if we've reached the threshold\n",
        "            if frequencies[word] == threshold:\n",
        "                vocabulary.update(tokenized_text)\n",
        "\n",
        "    return vocabulary\n",
        "  \n",
        "vocabulary = build_vocabulary(ds_train)\n",
        "vocab_file = open(\"vocabulary.obj\", \"wb\")\n",
        "pickle.dump(vocabulary, vocab_file)"
      ],
      "metadata": {
        "id": "YW839ZWQkYnh"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vocab_file = open(\"vocabulary.obj\", \"rb\")\n",
        "# vocabulary = pickle.load(vocab_file)"
      ],
      "metadata": {
        "id": "ZBP8AXgsmTn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizing"
      ],
      "metadata": {
        "id": "FBteuNFXmf-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = tfds.deprecated.text.TokenTextEncoder(\n",
        "    list(vocabulary), oov_token=\"<UNK>\", lowercase=True, tokenizer=tokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "t4Z2BWcbmYAe"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def my_encoder(text_tensor, label):\n",
        "    encoded_text = encoder.encode(text_tensor.numpy())\n",
        "    return encoded_text, label\n",
        "\n",
        "\n",
        "def encode_map_fn(line):\n",
        "    split_line = tf.strings.split(line, \",\", maxsplit=3)\n",
        "    label_str = split_line[1]  # 1, 0\n",
        "    review = \"sostoken \" + split_line[3] + \" eostoken\"\n",
        "    label = 1 if label_str == \"1\" else 0\n",
        "\n",
        "    (encoded_text, label) = tf.py_function(\n",
        "        my_encoder, inp=[review, label], Tout=(tf.int64, tf.int32),\n",
        "    )\n",
        "\n",
        "    encoded_text.set_shape([None])\n",
        "    label.set_shape([])\n",
        "    return encoded_text, label"
      ],
      "metadata": {
        "id": "AlRZMjk6mnCR"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "ds_train = ds_train.map(encode_map_fn, num_parallel_calls=AUTOTUNE).cache()\n",
        "ds_train = ds_train.shuffle(25000)\n",
        "ds_train = ds_train.padded_batch(32, padded_shapes=([None], ()))\n",
        "\n",
        "ds_test = ds_test.map(encode_map_fn)\n",
        "ds_test = ds_test.padded_batch(32, padded_shapes=([None], ()))\n",
        "\n",
        "print(type(ds_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVamq6ubm6_-",
        "outputId": "10c1a26e-1f9b-4aed-9d9b-037849e25fe6"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'tensorflow.python.data.ops.dataset_ops.PaddedBatchDataset'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RNN Model\n",
        "\n",
        "https://www.tensorflow.org/text/tutorials/text_classification_rnn\n",
        "\n",
        "Paper Uses - \n",
        "The architecture used in this paper is as follows: input layer, bi-directional hidden layer with 64 LSTM cells, dropout layer with p=0.5, bi-directional layer of 32 LSTM cells, dropout layer with p=0.5, dense layer of 20 hidden units with ReLU activation, softmax output layer. We initialize this network with random normal weights and train against the categorical crossentropy loss function with the adam optimizer. We use early stopping with a patience of 3 epochs."
      ],
      "metadata": {
        "id": "JPXHX5UV78ui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_graphs(history, metric):\n",
        "  plt.plot(history.history[metric])\n",
        "  plt.plot(history.history['val_'+metric], '')\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(metric)\n",
        "  plt.legend([metric, 'val_'+metric])"
      ],
      "metadata": {
        "id": "bx_aazpwWe1f"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential()\n",
        "model.add(layers.Bidirectional(layers.LSTM(64)))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Bidirectional(layers.LSTM(32)))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(20, activation='relu'))\n",
        "model.add(layers.Dense(2, kernel_initializer='normal', activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "Zm3FW0bAqUGb"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [EarlyStopping(monitor='val_loss', patience=3)]\n",
        "\n",
        "model.fit(\t\n",
        "      ds_train,\n",
        "      epochs=3, \n",
        "      callbacks=callbacks,\n",
        "      batch_size=1024)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 760
        },
        "id": "Bn39t9aqrp6o",
        "outputId": "c7df1a08-e794-4c81-9166-256e5957ba0e"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-61-8201fee8962e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m       \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m       \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m       verbose=0)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 859, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/input_spec.py\", line 214, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" '\n\n    ValueError: Exception encountered when calling layer \"sequential_4\" (type Sequential).\n    \n    Input 0 of layer \"bidirectional_4\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, None)\n    \n    Call arguments received:\n      • inputs=tf.Tensor(shape=(None, None), dtype=int64)\n      • training=True\n      • mask=None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qjBusEOGuHK3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}